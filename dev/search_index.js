var documenterSearchIndex = {"docs":
[{"location":"ensemble_builder/#Building-G-ensemble-matrix","page":"G Ensemble Builder","title":"Building G ensemble matrix","text":"warning: Warning\nTo enable this module, use using ClimaAnalysis or import ClimaAnalysis.\n\nnote: Prerequisites\nThis module assumes that you are using ObservationRecipe to make your observations and ClimaAnalysis to preprocess your simulation data. If this is not the case, this module is not for you.\n\nnote: Version of ClimaAnalysis\nThis module requires a version of ClimaAnalysis greater than v0.5.19.\n\nnote: Other documentation\nIt may be helpful to review the documentation for FlatVar in ClimaAnalysis and ObservationRecipe in ClimaCalibrate.\n\nTo help with constructing G ensemble matrix when using ObservationRecipe, ClimaCalibrate provides the struct GEnsembleBuilder and its related functions to easily create G ensemble matrix using the metadata stored in the observation. The metadata stores a rich amount of information that enables comprehensive validation and checking between simulation and observational data.\n\nThe metadata stored in the observations enable an automatic process of flattening or vectorizing your OutputVar and filling out your G ensemble matrix with consistency checks between the simulation data and observational data. This eliminates user errors, such as checking for the ordering of the dimensions when flattening the OutputVar, checking that the dimensions are consistent, and checking the units between observational and simulation data.","category":"section"},{"location":"ensemble_builder/#GEnsembleBuilder","page":"G Ensemble Builder","title":"GEnsembleBuilder","text":"To construct a GEnsembleBuilder, you pass the EKP.EnsembleKalmanProcess object to the constructor.\n\nimport ClimaCalibrate\nimport ClimaAnalysis # needed to enable extension\nimport ClimaCalibrate.EnsembleBuilder\n\n# ekp is a EnsembleKalmanProcesses.EnsembleKalmanProcess object\ng_ens_builder = EnsembleBuilder.GEnsembleBuilder(ekp)\n\nThen, in your observation map of your calibration, you should preprocess your OutputVars using ClimaAnalysis. For a CliMA simulation, this involves using SimDir to load the NetCDF files as OutputVars and preprocessing them, so that they match the OutputVars that are used to create the observations.\n\nnote: Short names\nUsing GEnsembleBuilder requires attaching a short name to all OutputVars for both simulation and observational data. The empty string cannot be used as a short name. Without the short name, it is not possible to determine which simulation data match with which observational data. Furthermore, the short names should be unique. For example, if you are calibrating monthly minimum and maximum of precipitation, then the short name for the monthly minimum can be pr_min and the short name for the monthly maximum can be pr_max.\n\nIn particular, the OutputVars from the simulation data should match the OutputVars from the observations from\n\nthe short name (see ClimaAnalysis.short_name),\nthe non-temporal dimensions,\nthe dates of the simulation data includes all the dates of one or more metadata in the observations,\nthe units of the variables.\n\nFor more information about the checks that are performed, see the Checkers section.\n\ninfo: Spinup and windowing times\nInternally, the correct dates are matched between the observational and simulation data. As a result, you do not need to window the times (e.g. when removing spinup) to match the times of the observations.\n\nwarning: Matching dates\nThere are no checks for how dates are matched which can easily lead to errors. For example, if the simulation data contain monthly averages and metadata track seasonal averages, then no error is thrown, because all dates in metadata are in all the dates in var.For these reasons, it is recommended to pass SequentialIndicesChecker to the checkers keyword argument of fill_g_ens_col! which will check that the dates used to fill the G ensemble matrix correspond to sequential indices in the simulation data.\n\nAfter preprocessing the OutputVars, you can call fill_g_ens_col! to fill the G ensemble matrix using the OutputVars.\n\n# -- preprocessing OutputVars from the first ensemble member--\n# var1, var2, and var3 are OutputVars of different quantities\n# The second argument is which column of the G ensemble matrix to fill out\nvars = (var1, var2, var3)\nfor var in vars\n    EnsembleBuilder.fill_g_ens_col!(g_ens_builder, 1, var)\nend\n\nIn this example, all the OutputVars of a single ensemble member are preprocessed and pass to fill_g_ens_col! in a loop. On the other hand, one can preprocess a single OutputVar, pass it to fill_g_ens_col!, and repeat for all the other OutputVars. We recommend the latter approach as loading all the OutputVars simultaneously can consume a lot of memory.\n\nnote: Unused OutputVars\nIf a OutputVar is passed to fill_g_ens_col! and it is not used, an error will not be thrown, but a warning will be thrown instead.\n\nNext, you can check if the G ensemble matrix is filled out with is_complete which returns true if the G ensemble matrix is completed and false otherwise.\n\nEnsembleBuilder.is_complete(g_ens_builder)\n\nIf this returns false, then you should review the warnings to determine why a OutputVar is not used to fill out the G ensemble matrix.\n\nFinally, you can get the G ensemble matrix with get_g_ensemble and return it from ClimaCalibrate.observation_map.\n\ng_ens = EnsembleBuilder.get_g_ensemble(g_ens_builder)","category":"section"},{"location":"ensemble_builder/#Checkers","page":"G Ensemble Builder","title":"Checkers","text":"To determine whether a OutputVar matches with a metadata, GEnsembleBuilder uses Checkers to check and compare the contents of a OutputVar and with that of the metadata. For example, the short names are checked between the OutputVar and metadata with ShortNameChecker.","category":"section"},{"location":"ensemble_builder/#Built-in-checkers","page":"G Ensemble Builder","title":"Built-in checkers","text":"ClimaCalibrate provides several built-in checkers:\n\nShortNameChecker: Check that the short names match between OutputVar and metadata\nDimNameChecker: Check the type of dimensions are the same\nUnitsChecker: Check the variable units are the same\nDimUnitsChecker: Check the units of the dimensions are the same\nDimValuesChecker: Check the values of the dimensions are the same\nSequentialIndicesChecker: Check the indices of the dates of the simulation data corresponding to the dates of the metadata is sequential.\nSignChecker: Check that the proportion of positive values in the simulation data and observational data are approximately equal (within a default threshold of 0.05).\n\nBy default, GEnsembleBuilder uses the first five checkers to validate compatibility between . You can also provide additional checkers using the checkers keyword argument in fill_g_ens_col!:\n\n# Use additional checker for sequential indices\nEnsembleBuilder.fill_g_ens_col!(\n    g_ens_builder,\n    1, \n    var,\n    checkers = (SequentialIndicesChecker(),)\n)","category":"section"},{"location":"ensemble_builder/#Implementing-custom-checkers","page":"G Ensemble Builder","title":"Implementing custom checkers","text":"To create a custom checker, define a struct that inherits from AbstractChecker and implement the check method:\n\nimport ClimaCalibrate.Checker\n\n# Define your custom checker\nstruct NothingChecker <: Checker.AbstractChecker end\n\n# Implement the check method\nfunction Checker.check(\n    ::NothingChecker,\n    var,\n    metadata;\n    data = nothing,\n    verbose = false,\n)\n    verbose && @info \"This is always true.\"\n    return true\nend\n\nThe Checker.check function should\n\naccept a checker instance, an OutputVar, and Metadata,\nreturn true if the check passes, false otherwise,\nand optionally log informative messages when verbose = true.\n\nFor more information about OutputVar and Metadata, see the ClimaAnalysis documentation.","category":"section"},{"location":"observations/#Observations","page":"Observations","title":"Observations","text":"Robust observations and accurate error covariances are essential for successful calibration. When calibrating climate models, it is advisable to use long-term climate statistics, such as monthly or seasonal averages, to reduce the influence of internal variability. This results in a more stable and representative target for inversion.\n\nEnsembleKalmanProcesses.jl provides several containers for managing observations, with documentation provided here. As inputs to a calibration, observations can consist of a Vector, an EKP.Observation (a single observation), or an EKP.ObservationSeries (many observations).\n\nTo iterate through an EKP.ObservationSeries, you must provide a minibatcher. This package provides two helper functions to faciliate the creation of simple batches:\n\nClimaCalibrate.minibatcher_over_samples takes in samples or (a number of samples) and a batch size and returns a minibatcher which divides the samples into the batch size, dropping remaining samples.\nClimaCalibrate.observation_series_from_samples takes in a vector of Observations and a batch size and returns an ObservationSeries with a minibatcher.","category":"section"},{"location":"precompilation/#Using-PrecompileTools-for-faster-model-runs","page":"Using PrecompileTools for faster model runs","title":"Using PrecompileTools for faster model runs","text":"PrecompileTools.jl enables developers to force the Julia compiler to save more code to disk, preventing re-compilation in the future.\n\nFor ClimaCalibrate, this is useful under certain conditions:\n\nThe atmosphere model configuration is set and will not change often. This is because the model configuration specifies things like the floating-point type and callbacks, which affect the MethodInstances that get precompiled. Generically precompiling ClimaAtmos would take much too long to be useful.\nThe model runtime is short compared to the compile time. If the model runtime is an order of magnitude or more than the compilation time, any benefit from reduced compilation time will be trivial.","category":"section"},{"location":"precompilation/#How-do-I-precompile-my-configuration?","page":"Using PrecompileTools for faster model runs","title":"How do I precompile my configuration?","text":"The easiest way is by copying and pasting the code snippet below into src/ClimaCalibrate.jl. This will precompile the model step and all callbacks for the given configuration.\n\nusing PrecompileTools\nimport SciMLBase\nimport ClimaAtmos as CA\nimport YAML\n\n@setup_workload begin\n    config_file = Dict(\"FLOAT_TYPE\" => \"Float64\")\n    @compile_workload begin\n        config = CA.AtmosConfig(config_dict)\n        simulation = CA.get_simulation(config)\n        (; integrator) = simulation\n        SciMLBase.step!(integrator)\n        CA.call_all_callbacks!(integrator)\n    end\nend","category":"section"},{"location":"submit_scripts/#Job-Submission-Scripts-for-HPC-Clusters","page":"Submission Scripts","title":"Job Submission Scripts for HPC Clusters","text":"This page provides concrete examples and best practices for running calibrations on HPC clusters using ClimaCalibrate.jl. The examples assume basic familiarity with either Slurm or PBS job schedulers.","category":"section"},{"location":"submit_scripts/#Overview","page":"Submission Scripts","title":"Overview","text":"ClimaCalibrate.jl supports two main approaches for running calibrations on HPC clusters:\n\nWorkerBackend: Uses Julia's distributed computing capabilities with workers managed by the job scheduler\nHPC Backends: Directly submits individual model runs as separate jobs to the scheduler\n\nThe choice between these approaches depends on your cluster's resource allocation policies and your model's computational requirements. For more information, see the Backends page.","category":"section"},{"location":"submit_scripts/#WorkerBackend-on-a-Slurm-cluster","page":"Submission Scripts","title":"WorkerBackend on a Slurm cluster","text":"When using WorkerBackend on a Slurm cluster, allocate resources at the top level since Slurm allows nested resource allocations. Each worker will inherit one task from the Slurm allocation.\n\n#!/bin/bash\n#SBATCH --job-name=slurm_calibration\n#SBATCH --output=calibration_%j.out\n#SBATCH --time=12:00:00\n#SBATCH --ntasks=5\n#SBATCH --cpus-per-task=4\n#SBATCH --gpus-per-task=1\n#SBATCH --mem=8G\n\n# Set environment variables for CliMA\nexport CLIMACOMMS_DEVICE=\"CUDA\"\nexport CLIMACOMMS_CONTEXT=\"SINGLETON\"\n\n# Load required modules\nmodule load climacommon\n\n# Build and run the Julia code\njulia --project=calibration -e 'using Pkg; Pkg.instantiate(;verbose=true)'\njulia --project=calibration calibration_script.jl\n\nKey points:\n\n--ntasks=5: Requests 5 tasks, each worker gets one task\n--cpus-per-task=4: Each worker gets 4 CPU cores\n--gpus-per-task=1: Each worker gets 1 GPU\nUses %j in output/error file names to interpolate the job ID","category":"section"},{"location":"submit_scripts/#WorkerBackend-on-a-PBS-cluster","page":"Submission Scripts","title":"WorkerBackend on a PBS cluster","text":"Since PBS does not support nested resource allocations, request minimal resources for the top-level script. Each worker will acquire its own resource allocation through the PBSManager.\n\n#!/bin/bash\n#PBS -N pbs_calibration\n#PBS -o calibration_${PBS_JOBID}.out\n#PBS -l walltime=12:00:00\n#PBS -l select=1:ncpus=1:mem=2GB\n\n# Set environment variables for CliMA\nexport CLIMACOMMS_DEVICE=\"CUDA\"\nexport CLIMACOMMS_CONTEXT=\"SINGLETON\"\n\n# Set temporary directory\nexport TMPDIR=$SCRATCH/tmp && mkdir -p $TMPDIR\n\n# Load required modules\nmodule load climacommon\n\n# Build and run the Julia code\njulia --project=calibration -e 'using Pkg; Pkg.instantiate(;verbose=true)'\njulia --project=calibration calibration_script.jl\n\nKey points:\n\nRequests only 1 CPU core for the main script\nWorkers will be launched as separate PBS jobs with their own resource allocations\nUses ${PBS_JOBID} to include the job ID in output file names","category":"section"},{"location":"submit_scripts/#HPC-Backend-Approach","page":"Submission Scripts","title":"HPC Backend Approach","text":"HPC backends directly submit individual forward model runs as separate jobs to the scheduler. This approach is ideal when:\n\nYour forward model requires multiple CPU cores or GPUs\nYou need fine-grained control over resource allocation per model run\nYour cluster doesn't support nested allocations\n\nSince each model run consists of an independent resource allocation, minimal resources are needed to run the top-level calibration script. For a slurm cluster, here is a minimal submission script:\n\n#!/bin/bash\n#SBATCH --job-name=slurm_calibration\n#SBATCH --output=calibration_%j.out\n#SBATCH --time=12:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\n# Load required modules\nmodule load climacommon\n\n# Build and run the Julia code\njulia --project=calibration -e 'using Pkg; Pkg.instantiate(;verbose=true)'\njulia --project=calibration calibration_script.jl\n\nFor a PBS cluster, the script in the WorkerBackend section can be reused since it already specifies a minimal resource allocation.","category":"section"},{"location":"submit_scripts/#Resource-Configuration","page":"Submission Scripts","title":"Resource Configuration","text":"","category":"section"},{"location":"submit_scripts/#CPU-Only-Jobs","page":"Submission Scripts","title":"CPU-Only Jobs","text":"For CPU-only forward models:\n\nhpc_kwargs = Dict(\n    :time => 30,\n    :ntasks => 1,\n    :cpus_per_task => 8,\n    :mem => \"16G\"\n)","category":"section"},{"location":"submit_scripts/#GPU-Jobs","page":"Submission Scripts","title":"GPU Jobs","text":"For GPU-accelerated forward models:\n\nhpc_kwargs = Dict(\n    :time => 60,\n    :ntasks => 1,\n    :cpus_per_task => 4,\n    :gpus_per_task => 1,\n    :mem => \"32G\"\n)","category":"section"},{"location":"submit_scripts/#Multi-Node-Jobs","page":"Submission Scripts","title":"Multi-Node Jobs","text":"For models requiring multiple nodes:\n\nhpc_kwargs = Dict(\n    :time => 120,\n    :ntasks => 16,\n    :cpus_per_task => 4,\n    :nodes => 4,\n    :mem => \"64G\"\n)","category":"section"},{"location":"submit_scripts/#Environment-Variables","page":"Submission Scripts","title":"Environment Variables","text":"Set these environment variables in your submission script:\n\nCLIMACOMMS_DEVICE: Set to \"CUDA\" for GPU runs or \"CPU\" for CPU-only runs\nCLIMACOMMS_CONTEXT: Set to \"SINGLETON\" for WorkerBackend. The context is automatically set to \"MPI\" for HPC backends","category":"section"},{"location":"submit_scripts/#Troubleshooting","page":"Submission Scripts","title":"Troubleshooting","text":"","category":"section"},{"location":"submit_scripts/#Common-Issues","page":"Submission Scripts","title":"Common Issues","text":"Worker Timeout: Increase ENV[\"JULIA_WORKER_TIMEOUT\"] in your Julia session if workers are timing out\nMemory Issues: Monitor memory usage and adjust --mem or -l mem accordingly. \nGPU Allocation: Ensure --gpus-per-task or -l select is set correctly\nModule Conflicts: Use module purge and ensure your MODULEPATH is set before loading required modules","category":"section"},{"location":"submit_scripts/#Debugging-Commands","page":"Submission Scripts","title":"Debugging Commands","text":"# Check job status (Slurm)\nsqueue -u $USER\n\n# Check job status (PBS)\nqstat -u $USER\n\n# View job logs\ntail -f calibration_<jobid>.out\n\n# Check resource usage\nseff <jobid>  # Slurm\nqstat -f <jobid>  # PBS","category":"section"},{"location":"emulate_sample/#Emulate-and-Sample","page":"Emulate and Sample","title":"Emulate and Sample","text":"Once you have run a successful calibration, we can fit an emulator to the resulting input/output pairs.\n\nFirst, import the necessary packages:\n\nimport JLD2\n\nusing CalibrateEmulateSample.Emulators\nusing CalibrateEmulateSample.MarkovChainMonteCarlo\n\nimport EnsembleKalmanProcesses as EKP\nusing EnsembleKalmanProcesses.ParameterDistributions\nusing EnsembleKalmanProcesses.TOMLInterface\n\nimport ClimaCalibrate as CAL\n\nNext, load in the data, EKP object, and prior distribution. These values are taken from the Held-Suarez perfect model experiment in ClimaAtmos.\n\nasset_path = joinpath(\n    pkgdir(CAL),\n    \"docs\",\n    \"src\",\n    \"assets\")\n\nekp = JLD2.load_object(joinpath(asset_path, \"emulate_example_ekiobj.jld2\"))\ny_obs = ekp.obs_mean\ny_noise_cov = ekp.obs_noise_cov\ninitial_params = [EKP.get_u_final(ekp)[1]]\n\nprior_path = joinpath(asset_path, \"emulate_example_prior.toml\")\nprior = CAL.get_prior(prior_path)\n\nGet the input-output pairs which will be used to train the emulator.  The inputs are the parameter values, and the outputs are the result of the observation map.  In thise case, the outputs are the average air temperature at roughly 500 meters.\n\ninput_output_pairs = CAL.get_input_output_pairs(ekp)\n\nNext, create the Gaussian Process-based emulator and Markov chain.  The samples from the chain can be used in future predictive model runs with the same configuration. The posterior distribution can be saved to a JLD2 file using save_posterior. Samples can be extracted from the posterior using ClimaParams.\n\nemulator = CAL.gp_emulator(input_output_pairs, y_noise_cov)\n(; mcmc, chain) = CAL.sample(emulator, y_obs, prior, initial_params)\nconstrained_posterior = CAL.save_posterior(mcmc, chain; filename = \"samples.jld2\")\ndisplay(chain)","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Model-Interface","page":"API","title":"Model Interface","text":"","category":"section"},{"location":"api/#Worker-Interface","page":"API","title":"Worker Interface","text":"","category":"section"},{"location":"api/#Backend-Interface","page":"API","title":"Backend Interface","text":"","category":"section"},{"location":"api/#Job-Scheduler","page":"API","title":"Job Scheduler","text":"","category":"section"},{"location":"api/#EnsembleKalmanProcesses-Interface","page":"API","title":"EnsembleKalmanProcesses Interface","text":"","category":"section"},{"location":"api/#Observation-Recipe-Interface","page":"API","title":"Observation Recipe Interface","text":"","category":"section"},{"location":"api/#Ensemble-Builder-Interface","page":"API","title":"Ensemble Builder Interface","text":"","category":"section"},{"location":"api/#Checker-Interface","page":"API","title":"Checker Interface","text":"","category":"section"},{"location":"api/#ClimaCalibrate.forward_model","page":"API","title":"ClimaCalibrate.forward_model","text":"forward_model(iteration, member)\n\nExecute the forward model simulation with the given configuration.\n\nThis function must be overridden by a component's model interface and  should set things like the parameter path and other member-specific settings.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.observation_map","page":"API","title":"ClimaCalibrate.observation_map","text":"observation_map(iteration)\n\nRuns the observation map for the specified iteration. This function must be implemented for each calibration experiment.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.analyze_iteration","page":"API","title":"ClimaCalibrate.analyze_iteration","text":"analyze_iteration(ekp, g_ensemble, prior, output_dir, iteration)\n\nAfter updating the ensemble and before starting the next iteration, analyze_iteration is evaluated.\n\nThis function is optional to implement.\n\nFor example, one may want to print information from the eki object or plot g_ensemble.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.postprocess_g_ensemble","page":"API","title":"ClimaCalibrate.postprocess_g_ensemble","text":"postprocess_g_ensemble(ekp, g_ensemble, prior, output_dir, iteration)\n\nPostprocess g_ensemble after evaluating the observation map and before updating the ensemble.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.add_workers","page":"API","title":"ClimaCalibrate.add_workers","text":"add_workers(\n    nworkers;\n    device = :gpu,\n    cluster = :auto,\n    time = DEFAULT_WALLTIME,\n    kwargs...\n)\n\nAdd nworkers worker processes to the current Julia session, automatically detecting and configuring for the available computing environment.\n\nArguments\n\nnworkers::Int: The number of worker processes to add.\ndevice::Symbol = :gpu: The target compute device type, either :gpu (1 GPU, 4 CPU cores) or :cpu (1 CPU core).\ncluster::Symbol = :auto: The cluster management system to use. Options:\n:auto: Auto-detect available cluster environment (SLURM, PBS, or local)\n:slurm: Force use of SLURM scheduler\n:pbs: Force use of PBS scheduler\n:local: Force use of local processing (standard addprocs)\ntime::Int = DEFAULT_WALLTIME: Walltime in minutes, will be formatted appropriately for the cluster system\nkwargs: Other kwargs can be passed directly through to addprocs.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.WorkerBackend","page":"API","title":"ClimaCalibrate.WorkerBackend","text":"WorkerBackend\n\nUsed to run calibrations on Distributed.jl's workers. For use on a Slurm cluster, see SlurmManager.\n\nKeyword Arguments for WorkerBackend\n\nfailure_rate::Float64: The threshold for the percentage of workers that can fail before an iteration is stopped. The default is 0.5.\nworker_pool: A worker pool created from the workers available.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.SlurmManager","page":"API","title":"ClimaCalibrate.SlurmManager","text":"SlurmManager(ntasks=get(ENV, \"SLURM_NTASKS\", 1))\n\nThe ClusterManager for Slurm clusters, taking in the number of tasks to request with srun.\n\nTo execute the srun command, run addprocs(SlurmManager(ntasks))\n\nKeyword arguments can be passed to srun: addprocs(SlurmManager(ntasks), gpus_per_task=1)\n\nBy default the workers will inherit the running Julia environment.\n\nTo run a calibration, call calibrate(WorkerBackend(), ...)\n\nTo run functions on a worker, call remotecall(func, worker_id, args...)\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.PBSManager","page":"API","title":"ClimaCalibrate.PBSManager","text":"PBSManager(ntasks)\n\nThe ClusterManager for PBS/Torque clusters, taking in the number of tasks to request with qsub.\n\nTo execute the qsub command, run addprocs(PBSManager(ntasks)).  Unlike the SlurmManager, this will not nest scheduled jobs, but will acquire new resources.\n\nKeyword arguments can be passed to qsub: addprocs(PBSManager(ntasks), nodes=2)\n\nBy default, the workers will inherit the running Julia environment.\n\nTo run a calibration, call calibrate(WorkerBackend(), ...)\n\nTo run functions on a worker, call remotecall(func, worker_id, args...)\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.set_worker_loggers","page":"API","title":"ClimaCalibrate.set_worker_loggers","text":"set_worker_loggers(workers = workers())\n\nSet the global logger to a simple file logger for the given workers.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.map_remotecall_fetch","page":"API","title":"ClimaCalibrate.map_remotecall_fetch","text":"map_remotecall_fetch(f::Function, args...; workers = workers())\n\nCall function f from each worker and wait for the results to return.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.foreach_remotecall_wait","page":"API","title":"ClimaCalibrate.foreach_remotecall_wait","text":"foreach_remotecall_wait(f::Function, args...; workers = workers())\n\nCall function f from each worker.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.calibrate","page":"API","title":"ClimaCalibrate.calibrate","text":"calibrate(\n    ensemble_size::Int,\n    n_iterations::Int,\n    observations,\n    noise,\n    prior,\n    output_dir;\n    backend_kwargs::NamedTuple,\n    ekp_kwargs...,\n)\n\nRun a calibration using a backend constructed from backend_kwargs and a EKP.EnsembleKalmanProcess constructed from ekp_kwargs.\n\nSee the backend's documentation for the available keyword arguments.\n\n\n\n\n\ncalibrate(backend, ekp::EnsembleKalmanProcess, ensemble_size, n_iterations, prior, output_dir)\ncalibrate(backend, ensemble_size, n_iterations, observations, noise, prior, output_dir; ekp_kwargs...)\n\nRun a full calibration on the given backend.\n\nIf the EKP struct is not given, it will be constructed upon initialization. While EKP keyword arguments are passed through to the EKP constructor, if using many keywords it is recommended to construct the EKP object and pass it into calibrate.\n\nAvailable Backends: WorkerBackend, CaltechHPCBackend, ClimaGPUBackend, DerechoBackend, JuliaBackend.\n\nDerecho, ClimaGPU, and CaltechHPC backends are designed to run on a specific high-performance computing cluster. WorkerBackend uses Distributed.jl to run the forward model on workers.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.JuliaBackend","page":"API","title":"ClimaCalibrate.JuliaBackend","text":"JuliaBackend\n\nThe simplest backend, used to run a calibration in Julia without any parallelization.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.HPCBackend","page":"API","title":"ClimaCalibrate.HPCBackend","text":"HPCBackend <: AbstractBackend\n\nAll concrete types of HPCBackend share the same keyword arguments for the constructors.\n\nKeyword Arguments for HPC backends\n\nhpc_kwargs::Dict{Symbol, String}: Dictionary of arguments passed to the job scheduler (e.g., Slurm or PBS). You may find the function kwargs helpful to construct hpc_kwargs.\nverbose::Bool: Enable verbose logging output. The default is false.\nexperiment_dir::String: Directory containing the experiment's Project.toml file. The default is the current project directory.\nmodel_interface::String: Absolute path to the model interface file that defines how to run the forward model. The default is abspath(joinpath(project_dir(), \"..\", \"..\", \"model_interface.jl\")).\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.DerechoBackend","page":"API","title":"ClimaCalibrate.DerechoBackend","text":"DerechoBackend\n\nUsed for NSF NCAR's Derecho supercomputing system.\n\nSee HPCBackend for the keyword arguments to construct a DerechoBackend.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.CaltechHPCBackend","page":"API","title":"ClimaCalibrate.CaltechHPCBackend","text":"CaltechHPCBackend\n\nUsed for Caltech's high-performance computing cluster.\n\nSee HPCBackend for the keyword arguments to construct a CaltechHPCBackend.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.ClimaGPUBackend","page":"API","title":"ClimaCalibrate.ClimaGPUBackend","text":"ClimaGPUBackend\n\nUsed for CliMA's private GPU server.\n\nSee HPCBackend for the keyword arguments to construct a ClimaGPUBackend.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.GCPBackend","page":"API","title":"ClimaCalibrate.GCPBackend","text":"GCPBackend\n\nUsed for CliMA's private GPU server.\n\nSee HPCBackend for the keyword arguments to construct a GCPBackend.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.get_backend","page":"API","title":"ClimaCalibrate.get_backend","text":"get_backend()\n\nGet ideal backend for deploying forward model runs. Each backend is found via gethostname(). Defaults to JuliaBackend if none is found.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.model_run","page":"API","title":"ClimaCalibrate.model_run","text":"model_run(backend, iter, member, output_dir, project_dir, module_load_str; exeflags)\n\nConstruct and execute a command to run a single forward model on a given job scheduler.\n\nUses the given backend to run slurm_model_run or pbs_model_run.\n\nArguments:\n\niter: Iteration number\nmember: Member number\noutput_dir: Calibration experiment output directory\nproject_dir: Directory containing the experiment's Project.toml\nmodule_load_str: Commands which load the necessary modules\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.module_load_string","page":"API","title":"ClimaCalibrate.module_load_string","text":"module_load_string(backend)\n\nReturn a string that loads the correct modules for a given backend when executed via bash.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.wait_for_jobs","page":"API","title":"ClimaCalibrate.wait_for_jobs","text":"wait_for_jobs(jobids, output_dir, iter, experiment_dir, model_interface, module_load_str, model_run_func; verbose, hpc_kwargs, reruns=1)\n\nWait for a set of jobs to complete. If a job fails, it will be rerun up to reruns times.\n\nThis function monitors the status of multiple jobs and handles failures by rerunning the failed jobs up to the specified number of reruns. It logs errors and job completion status, ensuring all jobs are completed before proceeding.\n\nArguments:\n\njobids: Vector of job IDs.\noutput_dir: Directory for output.\niter: Iteration number.\nexperiment_dir: Directory for the experiment.\nmodel_interface: Interface to the model.\nmodule_load_str: Commands to load necessary modules.\nmodel_run_func: Function to run the model.\nverbose: Print detailed logs if true.\nhpc_kwargs: HPC job parameters.\nreruns: Number of times to rerun failed jobs.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.log_member_error","page":"API","title":"ClimaCalibrate.log_member_error","text":"log_member_error(output_dir, iteration, member, verbose=false)\n\nLog a warning message when an error occurs. If verbose, includes the ensemble member's output.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.kill_job","page":"API","title":"ClimaCalibrate.kill_job","text":"kill_job(jobid::SlurmJobID)\nkill_job(jobid::PBSJobID)\n\nEnd a running job, catching errors in case the job can not be ended.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.job_status","page":"API","title":"ClimaCalibrate.job_status","text":"job_status(job_id)\n\nParse the slurm job_id's state and return one of three status symbols: :PENDING, :RUNNING, or :COMPLETED.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.kwargs","page":"API","title":"ClimaCalibrate.kwargs","text":"kwargs(; kwargs...)\n\nCreate a dictionary from keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.slurm_model_run","page":"API","title":"ClimaCalibrate.slurm_model_run","text":"slurm_model_run(iter, member, output_dir, experiment_dir, model_interface, module_load_str; hpc_kwargs)\n\nConstruct and execute a command to run a single forward model on Slurm. Helper function for model_run.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.generate_sbatch_script","page":"API","title":"ClimaCalibrate.generate_sbatch_script","text":"generate_sbatch_script(iter, member, output_dir, experiment_dir, model_interface; module_load_str, hpc_kwargs, exeflags=\"\")\n\nGenerate a string containing an sbatch script to run the forward model. hpc_kwargs is turned into a series of sbatch directives using generate_sbatch_directives. module_load_str is used to load the necessary modules and can be obtained via module_load_string. exeflags is a string of flags to pass to the Julia executable (defaults to empty string).\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.generate_sbatch_directives","page":"API","title":"ClimaCalibrate.generate_sbatch_directives","text":"generate_sbatch_directives(hpc_kwargs)\n\nGenerate Slurm sbatch directives from HPC kwargs. \n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.submit_slurm_job","page":"API","title":"ClimaCalibrate.submit_slurm_job","text":"submit_slurm_job(sbatch_filepath; env=deepcopy(ENV))\n\nSubmit a job to the Slurm scheduler using sbatch, removing unwanted environment variables.\n\nUnset variables: \"SLURMMEMPERCPU\", \"SLURMMEMPERGPU\", \"SLURMMEMPER_NODE\"\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.pbs_model_run","page":"API","title":"ClimaCalibrate.pbs_model_run","text":"pbs_model_run(iter, member, output_dir, experiment_dir, model_interface, module_load_str; hpc_kwargs)\n\nConstruct and execute a command to run a single forward model on PBS Pro. Helper function for model_run.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.generate_pbs_script","page":"API","title":"ClimaCalibrate.generate_pbs_script","text":"generatepbsscript(         iter, member,         outputdir, experimentdir, modelinterface;         moduleloadstr, hpckwargs,     )\n\nGenerate a string containing a PBS script to run the forward model.\n\nReturns:\n\nqsub_contents::Function: A function generating the content of the PBS script based on the provided arguments.    This will run the contents of the julia_script, which have to be run from a file due to Derecho's set_gpu_rank.\njulia_script::String: The Julia script string to be executed by the PBS job. \n\nHelper function for pbs_model_run.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.submit_pbs_job","page":"API","title":"ClimaCalibrate.submit_pbs_job","text":"submit_pbs_job(sbatch_filepath; env=deepcopy(ENV))\n\nSubmit a job to the PBS Pro scheduler using qsub, removing unwanted environment variables.\n\nUnset variables: \"PBSMEMPERCPU\", \"PBSMEMPERGPU\", \"PBSMEMPER_NODE\", \"PYTHONHOME\", \"PYTHONPATH\", \"PYTHONUSERBASE\"\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.initialize","page":"API","title":"ClimaCalibrate.initialize","text":"initialize(eki::EKP.EnsembleKalmanProcess, prior, output_dir)\ninitialize(ensemble_size, observations, noise, prior, output_dir)\n\nInitialize a calibration, saving the initial parameter ensemble to a folder within output_dir.\n\nIf no EKP struct is given, construct an EKP struct and return it.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.save_G_ensemble","page":"API","title":"ClimaCalibrate.save_G_ensemble","text":"save_G_ensemble(output_dir::AbstractString, iteration, G_ensemble)\n\nSaves the ensemble's observation map output to the correct directory based on the provided configuration. Takes an output directory, iteration number, and the ensemble output to save.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.update_ensemble","page":"API","title":"ClimaCalibrate.update_ensemble","text":"update_ensemble(output_dir::AbstractString, iteration, prior)\n\nUpdates the EnsembleKalmanProcess object and saves the parameters for the next iteration.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.update_ensemble!","page":"API","title":"ClimaCalibrate.update_ensemble!","text":"update_ensemble!(ekp, G_ens, output_dir, iteration, prior)\n\nUpdates an EKP object with data G_ens, saving the object and final parameters to disk.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.observation_map_and_update!","page":"API","title":"ClimaCalibrate.observation_map_and_update!","text":"observation_map_and_update!(ekp, output_dir, iteration, prior)\n\nCompute the observation map and update the given EKP object.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.get_prior","page":"API","title":"ClimaCalibrate.get_prior","text":"get_prior(param_dict::AbstractDict; names = nothing)\nget_prior(prior_path::AbstractString; names = nothing)\n\nConstructs the combined prior distribution from a param_dict or a TOML configuration file specified by prior_path. If names is provided, only those parameters are used.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.get_param_dict","page":"API","title":"ClimaCalibrate.get_param_dict","text":"get_param_dict(distribution; names)\n\nGenerates a dictionary for parameters based on the specified distribution, assumed to be of floating-point type. If names is not provided, the distribution's names will be used.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.path_to_iteration","page":"API","title":"ClimaCalibrate.path_to_iteration","text":"path_to_iteration(output_dir, iteration)\n\nReturn the path to the directory for a given iteration within the specified output directory.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.path_to_ensemble_member","page":"API","title":"ClimaCalibrate.path_to_ensemble_member","text":"path_to_ensemble_member(output_dir, iteration, member)\n\nReturn the path to an ensemble member's directory for a given iteration and member number.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.path_to_model_log","page":"API","title":"ClimaCalibrate.path_to_model_log","text":"path_to_model_log(output_dir, iteration, member)\n\nReturn the path to an ensemble member's forward model log for a given iteration and member number.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.parameter_path","page":"API","title":"ClimaCalibrate.parameter_path","text":"parameter_path(output_dir, iteration, member)\n\nReturn the path to an ensemble member's parameter file.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.minibatcher_over_samples","page":"API","title":"ClimaCalibrate.minibatcher_over_samples","text":"minibatcher_over_samples(n_samples, batch_size)\n\nCreate a FixedMinibatcher that divides n_samples into batches of size batch_size.\n\nIf n_samples is not divisible by batch_size, the remaining samples will be dropped.\n\n\n\n\n\nminibatcher_over_samples(samples, batch_size)\n\nCreate a FixedMinibatcher that divides a vector of samples into batches of size batch_size.\n\nIf the number of samples is not divisible by batch_size, the remaining samples will be dropped.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.observation_series_from_samples","page":"API","title":"ClimaCalibrate.observation_series_from_samples","text":"observation_series_from_samples(samples, batch_size, names = nothing)\n\nCreate an EKP.ObservationSeries from a vector of EKP.Observation samples.\n\nIf the number of samples is not divisible by batch_size, the remaining samples will be dropped.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.load_latest_ekp","page":"API","title":"ClimaCalibrate.load_latest_ekp","text":"load_latest_ekp(output_dir)\n\nReturn the most recent EnsembleKalmanProcess struct from the given output directory.\n\nReturns nothing if no EKP structs are found.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.AbstractCovarianceEstimator","page":"API","title":"ClimaCalibrate.ObservationRecipe.AbstractCovarianceEstimator","text":"abstract type AbstractCovarianceEstimator end\n\nAn object that estimates the noise covariance matrix from observational data that is appropriate for a sample between start_date and end_date.\n\nAbstractCovarianceEstimator have to provide one function, ObservationRecipe.covariance.\n\nThe function has to have the signature\n\nObservationRecipe.covariance(\n    covar_estimator::AbstractCovarianceEstimator,\n    vars,\n    start_date,\n    end_date,\n)\n\nand return a noise covariance matrix.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.ObservationRecipe.ScalarCovariance","page":"API","title":"ClimaCalibrate.ObservationRecipe.ScalarCovariance","text":"ScalarCovariance <: AbstractCovarianceEstimator\n\nContain the necessary information to construct the scalar covariance matrix.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.ObservationRecipe.ScalarCovariance-Tuple{}","page":"API","title":"ClimaCalibrate.ObservationRecipe.ScalarCovariance","text":"ScalarCovariance(;\n    scalar = 1.0,\n    use_latitude_weights = false,\n    min_cosd_lat = 0.1,\n)\n\nCreate a ScalarCovariance which specifies how the covariance matrix should be formed. When used with ObservationRecipe.observation or ObservationRecipe.covariance, return a Diagonal matrix.\n\nKeyword arguments\n\nscalar: Scalar value to multiply the identity matrix by.\nuse_latitude_weights: If true, then latitude weighting is applied to the covariance matrix. Latitude weighting is multiplying the values along the diagonal of the covariance matrix by (1 / max(cosd(lat), min_cosd_lat)). See the keyword argument min_cosd_lat for more information.\nmin_cosd_lat: Control the minimum latitude weight when use_latitude_weights is true. The value for min_cosd_lat must be greater than zero as values close to zero along the diagonal of the covariance matrix can lead to issues when taking the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#ClimaCalibrate.ObservationRecipe.SeasonalDiagonalCovariance","page":"API","title":"ClimaCalibrate.ObservationRecipe.SeasonalDiagonalCovariance","text":"SeasonalDiagonalCovariance <: AbstractCovarianceEstimator\n\nContain the necessary information to construct a diagonal covariance matrix whose entries represents seasonal covariances from ClimaAnalysis.OutputVars.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.ObservationRecipe.SeasonalDiagonalCovariance-Tuple{}","page":"API","title":"ClimaCalibrate.ObservationRecipe.SeasonalDiagonalCovariance","text":"SeasonalDiagonalCovariance(model_error_scale = 0.0,\n                           regularization = 0.0,\n                           ignore_nan = true,\n                           use_latitude_weights = false,\n                           min_cosd_lat = 0.1)\n\nCreate a SeasonalDiagonalCovariance which specifies how the covariance matrix should be formed. When used with ObservationRecipe.observation or ObservationRecipe.covariance, return a Diagonal matrix.\n\nKeyword arguments\n\nmodel_error_scale: Noise from the model error added to the covariance matrix. This is (model_error_scale * seasonal_mean).^2, where seasonal_mean is the seasonal mean for each of the quantity for each of the season (DJF, MAM, JJA, SON).\nregularization: A diagonal matrix of the form regularization * I is added to the covariance matrix.\nignore_nan: If true, then NaNs are ignored when computing the covariance matrix. Otherwise, NaN are included in the intermediate calculation of the covariance matrix. Note that all NaNs are removed in the last step of forming the covariance matrix even if ignore_nan is false.\nuse_latitude_weights: If true, then latitude weighting is applied to the covariance matrix. Latitude weighting is multiplying the values along the diagonal of the covariance matrix by (1 / max(cosd(lat), min_cosd_lat)). See the keyword argument min_cosd_lat for more information.\nmin_cosd_lat: Control the minimum latitude weight when use_latitude_weights is true. The value for min_cosd_lat must be greater than zero as values close to zero along the diagonal of the covariance matrix can lead to issues when taking the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#ClimaCalibrate.ObservationRecipe.SVDplusDCovariance","page":"API","title":"ClimaCalibrate.ObservationRecipe.SVDplusDCovariance","text":"SVDplusDCovariance <: AbstractCovarianceEstimator\n\nContain the necessary information to construct a EKP.SVDplusD covariance matrix from ClimaAnalysis.OutputVars.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.ObservationRecipe.SVDplusDCovariance-Tuple{Any}","page":"API","title":"ClimaCalibrate.ObservationRecipe.SVDplusDCovariance","text":"SVDplusDCovariance(sample_date_ranges;\n                   model_error_scale = 0.0,\n                   regularization = 0.0,\n                   use_latitude_weights = false,\n                   min_cosd_lat = 0.1)\n\nCreate a SVDplusDCovariance which specifies how the covariance matrix should be formed. When used with ObservationRecipe.observation or ObservationRecipe.covariance, return a EKP.SVDplusD covariance matrix.\n\nnote: Recommended sample size\nFor sample_date_ranges, it is recommended that each sample contains data from a single year. For example, if the samples are created from time series data of seasonal averages, then each sample should contain all four seasons. Otherwise, the covariance matrix may not make sense. For example, if each sample contains two years of seasonally averaged data, then the sample mean is the seasonal mean of every other season across the years stacked vertically. For a concrete example, if the sample contain DJF for both 2010 and 2011. Then, the sample mean will be of mean of DJF 2010, 2012, and so on, and the mean of DJF 2011, 2013, and so on. As a result, if one were to use this covariance matrix with model_error_scale, the covariance matrix will not make sense.\n\nPositional arguments\n\nsample_date_ranges: The start and end dates of each samples. This is used to determine the sample from the time series data of the OutputVars. These dates must be present in all the OutputVars.\n\nKeyword arguments\n\nmodel_error_scale: Noise from the model error added to the covariance matrix. This is (model_error_scale * mean(samples, dims = 2)).^2, where mean(samples, dims = 2) is the mean of the samples.\nregularization: A diagonal matrix of the form regularization * I is added to the covariance matrix.\nuse_latitude_weights: If true, then latitude weighting is applied to the covariance matrix. Latitude weighting is multiplying the columns of the matrix of samples by 1 / sqrt(max(cosd(lat), 0.1)). See the keyword argument min_cosd_lat for more information.\nmin_cosd_lat: Control the minimum latitude weight when use_latitude_weights is true. The value for min_cosd_lat must be greater than zero as values close to zero along the diagonal of the covariance matrix can lead to issues when taking the inverse of the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#ClimaCalibrate.ObservationRecipe.covariance","page":"API","title":"ClimaCalibrate.ObservationRecipe.covariance","text":"covariance(covar_estimator::ScalarCovariance,\n           vars::Union{OutputVar, Iterable{OutputVar}},\n           start_date,\n           end_date)\n\nCompute the scalar covariance matrix.\n\nData from vars will not be used to compute the covariance matrix.\n\n\n\n\n\ncovariance(covar_estimator::SeasonalDiagonalCovariance,\n           vars::Union{OutputVar, Iterable{OutputVar}},\n           start_date,\n           end_date)\n\nCompute the noise covariance matrix of seasonal quantities from var that is appropriate for a sample of seasonal quantities across time for seasons between start_date and end_date.\n\nThe diagonal is computed from the variances of the seasonal quantities.\n\n\n\n\n\ncovariance(covar_estimator::SVDplusDCovariance,\n           vars::Union{OutputVar, Iterable{OutputVar}},\n           start_date,\n           end_date)\n\nCompute the EKP.SVDplusD covariance matrix appropriate for a sample with times between start_date and end_date.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.observation","page":"API","title":"ClimaCalibrate.ObservationRecipe.observation","text":"observation(covar_estimator::AbstractCovarianceEstimator,\n            vars,\n            start_date,\n            end_date;\n            name = nothing)\n\nReturn an EKP.Observation with a sample between the dates start_date and end_date, a covariance matrix defined by covar_estimator, name determined from the short names of vars, and metadata.\n\nnote: Metadata\nMetadata in EKP.observation is only added with versions of EnsembleKalmanProcesses later than v2.4.2.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.short_names","page":"API","title":"ClimaCalibrate.ObservationRecipe.short_names","text":"short_names(obs::EKP.Observation)\n\nGet the short names of the variables from the metadata in the EKP.Observation.\n\nIf the short name is not available, then nothing is returned instead.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.get_observations_for_nth_iteration","page":"API","title":"ClimaCalibrate.ObservationRecipe.get_observations_for_nth_iteration","text":"get_observations_for_nth_iteration(obs_series, N)\n\nFor the Nth iteration, get the observation(s) being processed.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.get_metadata_for_nth_iteration","page":"API","title":"ClimaCalibrate.ObservationRecipe.get_metadata_for_nth_iteration","text":"get_metadata_for_nth_iteration(obs_series, N)\n\nFor the Nth iteration, get the metadata of the observation(s) being processed.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.reconstruct_g_mean_final","page":"API","title":"ClimaCalibrate.ObservationRecipe.reconstruct_g_mean_final","text":"reconstruct_g_mean_final(ekp::EKP.EnsembleKalmanProcess)\n\nReconstruct the mean forward model evaluation at the last iteration as a vector of OutputVars.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.reconstruct_diag_cov","page":"API","title":"ClimaCalibrate.ObservationRecipe.reconstruct_diag_cov","text":"reconstruct_diag_cov(obs::EKP.Observation)\n\nReconstruct the diagonal of the covariance matrix in obs as a vector of OutputVars.\n\nThis function only supports observations that contain diagonal covariance matrices.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.reconstruct_vars","page":"API","title":"ClimaCalibrate.ObservationRecipe.reconstruct_vars","text":"reconstruct_vars(obs::EKP.Observation)\n\nReconstruct the OutputVars from the samples in obs.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.seasonally_aligned_yearly_sample_date_ranges","page":"API","title":"ClimaCalibrate.ObservationRecipe.seasonally_aligned_yearly_sample_date_ranges","text":"seasonally_aligned_yearly_sample_date_ranges(var::OutputVar)\n\nGenerate sample dates that conform to a seasonally aligned year from dates(var).\n\nA seasonally aligned year is defined to be from December to November of the following year.\n\nThis function is useful for finding the sample dates of samples consisting of all four seasons in a single year. For example, one can use this function to find the sample_date_ranges when constructing SVDplusDCovariance.\n\nnote: All four seasons in a year is not guaranteed\nThis function does not check whether the start and end dates of each sample contain all four seasons. A sample may be missing a season, especially at the beginning or end of the time series.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.ObservationRecipe.change_data_type","page":"API","title":"ClimaCalibrate.ObservationRecipe.change_data_type","text":"ObservationRecipe.change_data_type(var::OutputVar, data_type)\n\nReturn a OutputVar with data of type data_type.\n\nThis is useful if you want to make covariance matrix whose element type is data_type.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaAnalysisExt.GEnsembleBuilder","page":"API","title":"ClimaAnalysisExt.GEnsembleBuilder","text":"GEnsembleBuilder{FT <: AbstractFloat}\n\nAn object to help build G ensemble matrix by using the metadata stored in the EKP.EnsembleKalmanProcess object. Metadata must come from ClimaAnalysis.\n\nGEnsembleBuilder takes in preprocessed OutputVars and automatically construct the corresponding G ensemble matrix for the current iteration of the calibration.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.GEnsembleBuilder","page":"API","title":"ClimaCalibrate.EnsembleBuilder.GEnsembleBuilder","text":"GEnsembleBuilder(ekp::EKP.EnsembleKalmanProcess{FT})\n    where {FT <: AbstractFloat}\n\nConstruct a GEnsembleBuilder where the element type of the G ensemble matrix is FT.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.fill_g_ens_col!","page":"API","title":"ClimaCalibrate.EnsembleBuilder.fill_g_ens_col!","text":"EnsembleBuilder.fill_g_ens_col!(g_ens_builder::GEnsembleBuilder,\n                                col_idx,\n                                var::OutputVar;\n                                checkers = (),\n                                verbose = false)\n\nFill the col_idxth of the G ensemble matrix from the OutputVar var and ekp. If it was successful, return true, otherwise, return false.\n\nIt is assumed that the times or dates of a single OutputVar is a superset of the times or dates of one or more metadata in the minibatch.\n\nThis function relies on the short names in the metadata. This function will not behave correctly if the short names are mislabled or not present.\n\nFurthermore, this function assumes that all observations are generated using ObservationRecipe.Observation which guarantees that the metadata exists and the correct placement of metadata.\n\n\n\n\n\nEnsembleBuilder.fill_g_ens_col!(g_ens_builder::GEnsembleBuilder,\n                                col_idx,\n                                val::AbstractFloat)\n\nFill the col_idxth column of the G ensemble matrix with val.\n\nThis returns true.\n\nThis is useful if you want to completely fill a column of a G ensemble matrix with NaNs if a simulation crashed.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.is_complete","page":"API","title":"ClimaCalibrate.EnsembleBuilder.is_complete","text":"EnsembleBuilder.is_complete(g_ens_builder::GEnsembleBuilder)\n\nReturn true if all the entries of the G ensemble matrix is filled out and false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.get_g_ensemble","page":"API","title":"ClimaCalibrate.EnsembleBuilder.get_g_ensemble","text":"EnsembleBuilder.get_g_ensemble(g_ens_builder::GEnsembleBuilder)\n\nReturn the G ensemble matrix from g_ens_builder.\n\nThis function does not check that the G ensemble matrix is completed. See ClimaCalibrate.EnsembleBuilder.is_complete to check if the G ensemble matrix is completely filled out.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.ranges_by_short_name","page":"API","title":"ClimaCalibrate.EnsembleBuilder.ranges_by_short_name","text":"ranges_by_short_name(g_ens_builder::GEnsembleBuilder, short_name)\n\nReturn a vector of ranges for the G ensemble matrix that correspond with the short name.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.metadata_by_short_name","page":"API","title":"ClimaCalibrate.EnsembleBuilder.metadata_by_short_name","text":"metadata_by_short_name(g_ens_builder::GEnsembleBuilder, short_name)\n\nReturn a vector of metadata that correspond with short_name.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.EnsembleBuilder.missing_short_names","page":"API","title":"ClimaCalibrate.EnsembleBuilder.missing_short_names","text":"missing_short_names(g_ens_builder::GEnsembleBuilder, col_idx)\n\nReturn a set of the short names of the metadata that are not filled out for the col_idxth column of g_ens_builder.\n\n\n\n\n\n","category":"function"},{"location":"api/#ClimaCalibrate.Checker.AbstractChecker","page":"API","title":"ClimaCalibrate.Checker.AbstractChecker","text":"abstract type AbstractChecker end\n\nAn object that performs validation checks between the simulation data and metadata from observational data. This is used by GEnsembleBuilder to validate OutputVars from simulation data against the Metadata in the observations in the EnsembleKalmanProcess object.\n\nAn AbstractChecker must implement the Checker.check function.\n\nThe function must have the signature:\n\nimport ClimaCalibrate.Checker\nChecker.check(::YourChecker,\n              var::OutputVar,\n              metadata::Metadata;\n              data = nothing,\n              verbose = false)\n\nand return true or false.\n\nnote: What is var and metadata?\nFor more information about OutputVar and Metadata, see the ClimaAnalysis documentation.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.ShortNameChecker","page":"API","title":"ClimaCalibrate.Checker.ShortNameChecker","text":"struct ShortNameChecker <: AbstractChecker end\n\nA struct that checks the short name between simulation data and metadata.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.DimNameChecker","page":"API","title":"ClimaCalibrate.Checker.DimNameChecker","text":"struct DimNameChecker <: AbstractChecker end\n\nA struct that checks the dimension names between simulation data and metadata.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.DimUnitsChecker","page":"API","title":"ClimaCalibrate.Checker.DimUnitsChecker","text":"struct DimUnitsChecker <: AbstractChecker end\n\nA struct that checks the units of the dimensions between simulation data and metadata.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.UnitsChecker","page":"API","title":"ClimaCalibrate.Checker.UnitsChecker","text":"struct UnitsChecker <: AbstractChecker end\n\nA struct that checks the units between the simulation data and metadata.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.DimValuesChecker","page":"API","title":"ClimaCalibrate.Checker.DimValuesChecker","text":"struct DimValuesChecker <: AbstractChecker end\n\nA struct that checks the values of the dimensions between the simulation data and metadata.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.SequentialIndicesChecker","page":"API","title":"ClimaCalibrate.Checker.SequentialIndicesChecker","text":"struct SequentialIndicesChecker <: AbstractChecker end\n\nA struct that checks that the indices of the dates of the simulation data corresponding to the dates of the metadata is sequential.\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.SignChecker","page":"API","title":"ClimaCalibrate.Checker.SignChecker","text":"struct SignChecker{FT <: AbstractFloat} <: AbstractChecker\n\nA struct that checks that the proportion of positive values in the simulation data and observational data is roughly the same.\n\nTo change the default threshold of 0.05, you can pass a float to SignChecker.\n\nimport ClimaCalibrate\nsign_checker = ClimaCalibrate.Checker.SignChecker(0.01)\n\n\n\n\n\n","category":"type"},{"location":"api/#ClimaCalibrate.Checker.check","page":"API","title":"ClimaCalibrate.Checker.check","text":"check(checker::AbstractChecker,\n      var,\n      metadata;\n      data = nothing,\n      verbose = false)\n\nReturn true if the check passes, false otherwise.\n\nIf verbose=true, then provides information for why a check did not succeed.\n\n\n\n\n\nChecker.check(\n    ::ShortNameChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if var and metadata have the same short name, false otherwise.\n\n\n\n\n\nChecker.check(\n    ::DimNameChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if var and metadata have the same dimensions, false otherwise.\n\n\n\n\n\nChecker.check(\n    ::DimUnitsChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if the units of the dimensions in var and metadata are the same, false otherwise. This function assumes var and metadata have the same dimensions.\n\n\n\n\n\nChecker.check(\n    ::UnitsChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if var and metadata have the same units, false otherwise.\n\n\n\n\n\nChecker.check(\n    ::DimValuesMatch,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if the values of the dimensions in var and metadata are compatible for the purpose of filling out the G ensemble matrix, false otherwise.\n\nThe nontemporal dimensions are compatible if the values are approximately the same. The temporal dimensions are compatible if the temporal dimension of metadata is a subset of the temporal dimension of var.\n\n\n\n\n\nChecker.check(\n    ::SequentialIndicesChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data = nothing,\n    verbose = false,\n)\n\nReturn true if the dates of var map to sequential indices of the dates of metadata, false otherwise.\n\nnote: Use this check\nIt is recommended to always enable this check when possible.\n\nnote: Why use this check?\nThis check is helpful in ensuring that the dates are matched correctly between var and metadata. For example, without this check, if the simulation data contain monthly averages and metadata track seasonal averages, then no error is thrown, because all dates in metadata are in all the dates in var.\n\n\n\n\n\nChecker.check(\n    ::SignChecker,\n    var::OutputVar,\n    metadata::Metadata;\n    data,\n    verbose = false,\n)\n\nReturn true if the absolute difference of the proportion of positive values in var.data and the proportion of positive values in data is less than the threshold defined in SignChecker, false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"backends/#Backends","page":"Backends","title":"Backends","text":"ClimaCalibrate can scale calibrations on different distributed computing environments, referred to as backends. Each backend is optimized for specific use cases and computing resources. The backend system is implemented through Julia's multiple dispatch, allowing seamless switching between different computing environments.","category":"section"},{"location":"backends/#Available-Backends","page":"Backends","title":"Available Backends","text":"JuliaBackend: The simplest backend that runs everything serially on a single machine. Best for initial testing and small calibrations that do not require parallelization. \nWorkerBackend: Uses Julia's built-in distributed computing capabilities, assigning forward model runs to separate workers using Distributed.jl. Workers can be created using SlurmManager, Distributed.addprocs, or by initializing julia with the -p option: julia -p 2. Available workers can be accessed using Distributed.workers().\nHPC Cluster Backends: These backends schedule forward model runs on HPC clusters using Slurm or PBS.\nCaltechHPCBackend: Caltech's Resnick HPC cluster\nClimaGPUBackend: CliMA's private GPU server\nDerechoBackend: NSF NCAR Derecho supercomputing system.","category":"section"},{"location":"backends/#Choosing-the-Right-Backend","page":"Backends","title":"Choosing the Right Backend","text":"The right backend is largely determined by the computational cost of your forward model.\n\nIf your model is very simple or you are debugging, use the JuliaBackend.\n\nIf your model requires just one CPU core or GPU, the best backend is the WorkerBackend. \n\nIf your forward model requires parallelization across multiple cores or GPUs, choose one of the HPC Cluster backends. These allow you allocate more resources to each forward model using Slurm or PBS.","category":"section"},{"location":"backends/#Using-a-Backend","page":"Backends","title":"Using a Backend","text":"Backends are the first argument to the calibrate function, which runs iterations of the forward model, updating model parameter based on observations.","category":"section"},{"location":"literate_example/#Distributed-Calibration-Tutorial-Using-Julia-Workers","page":"Distributed Calibration Tutorial","title":"Distributed Calibration Tutorial Using Julia Workers","text":"This example will teach you how to use ClimaCalibrate to parallelize your calibration with workers. Workers are additional processes spun up to run code in a distributed fashion. In this tutorial, we will run ensemble members' forward models on different workers.\n\nThe example calibration uses CliMA's atmosphere model, ClimaAtmos.jl, in a column spatial configuration for 30 days to simulate outgoing radiative fluxes. Radiative fluxes are used in the observation map to calibrate the astronomical unit.\n\nFirst, we load in some necessary packages.\n\nusing Distributed\nimport ClimaCalibrate as CAL\nimport ClimaAnalysis: SimDir, get, slice, average_xy\nusing ClimaUtilities.ClimaArtifacts\nimport EnsembleKalmanProcesses: I, ParameterDistributions.constrained_gaussian\n\nNext, we add workers. These are primarily added by Distributed.addprocs or by starting Julia with multiple processes: julia -p <nprocs>.\n\naddprocs itself initializes the workers and registers them with the main Julia process, but there are multiple ways to call it. The simplest is just addprocs(nprocs), which will create new local processes on your machine. The other is to use SlurmManager, which will acquire and start workers on Slurm resources. You can use keyword arguments to specify the Slurm resources:\n\naddprocs(ClimaCalibrate.SlurmManager(nprocs), gpus_per_task = 1, time = \"01:00:00\")\n\nFor this example, we would add one worker if it was compatible with Documenter.jl:\n\naddprocs(1)\n\nWe can see the number of workers and their ID numbers:\n\nnworkers()\n\nworkers()\n\nWe can call functions on the worker using remotecall. We pass in the function name and the worker ID followed by the function arguments.\n\nremotecall_fetch(*, 1, 4, 4)\n\nClimaCalibrate uses this functionality to run the forward model on workers.\n\nSince the workers start in their own Julia sessions, we need to import packages and declare variables. Distributed.@everywhere executes code on all workers, allowing us to load the code that they need.\n\n@everywhere begin\n    output_dir = joinpath(\"output\", \"climaatmos_calibration\")\n    import ClimaCalibrate as CAL\n    import ClimaAtmos as CA\n    import ClimaComms\nend\noutput_dir = joinpath(\"output\", \"climaatmos_calibration\")\nmkpath(output_dir)\n\nFirst, we need to set up the forward model, which take in the sampled parameters, runs, and saves diagnostic output that can be processed and compared to observations. The forward model must override ClimaCalibrate.forward_model(iteration, member), since the workers will run this function in parallel.\n\nSince forward_model(iteration, member) only takes in the iteration and member numbers, so we need to use these as hooks to set the model parameters and output directory. Two useful functions:\n\npath_to_ensemble_member: Returns the ensemble member's output directory\nparameter_path: Returns the ensemble member's parameter file as specified by EKP.TOMLInterface\n\nThe forward model below is running ClimaAtmos.jl in a minimal column spatial configuration.\n\n@everywhere function CAL.forward_model(iteration, member)\n    config_dict = Dict(\n        \"dt\" => \"2000secs\",\n        \"t_end\" => \"30days\",\n        \"config\" => \"column\",\n        \"h_elem\" => 1,\n        \"insolation\" => \"timevarying\",\n        \"output_dir\" => output_dir,\n        \"output_default_diagnostics\" => false,\n        \"dt_rad\" => \"6hours\",\n        \"rad\" => \"clearsky\",\n        \"co2_model\" => \"fixed\",\n        \"log_progress\" => false,\n        \"diagnostics\" => [\n            Dict(\n                \"reduction_time\" => \"average\",\n                \"short_name\" => \"rsut\",\n                \"period\" => \"30days\",\n                \"writer\" => \"nc\",\n            ),\n        ],\n    )\n    #md # Set the output path for the current member\n    member_path = CAL.path_to_ensemble_member(output_dir, iteration, member)\n    config_dict[\"output_dir\"] = member_path\n\n    #md # Set the parameters for the current member\n    parameter_path = CAL.parameter_path(output_dir, iteration, member)\n    if haskey(config_dict, \"toml\")\n        push!(config_dict[\"toml\"], parameter_path)\n    else\n        config_dict[\"toml\"] = [parameter_path]\n    end\n\n    #md # Turn off default diagnostics\n    config_dict[\"output_default_diagnostics\"] = false\n\n    comms_ctx = ClimaComms.SingletonCommsContext()\n    atmos_config = CA.AtmosConfig(config_dict; comms_ctx)\n    simulation = CA.get_simulation(atmos_config)\n    CA.solve_atmos!(simulation)\n    return simulation\nend\n\nNext, the observation map is required to process a full ensemble of model output for the ensemble update step. The observation map just takes in the iteration number, and always outputs an array. For observation map output G_ensemble, G_ensemble[:, m] must the output of ensemble member m. This is needed for compatibility with EnsembleKalmanProcesses.jl.\n\nconst days = 86_400\nfunction CAL.observation_map(iteration)\n    single_member_dims = (1,)\n    G_ensemble = Array{Float64}(undef, single_member_dims..., ensemble_size)\n\n    for m in 1:ensemble_size\n        member_path = CAL.path_to_ensemble_member(output_dir, iteration, m)\n        simdir_path = joinpath(member_path, \"output_active\")\n        if isdir(simdir_path)\n            simdir = SimDir(simdir_path)\n            G_ensemble[:, m] .= process_member_data(simdir)\n        else\n            G_ensemble[:, m] .= NaN\n        end\n    end\n    return G_ensemble\nend\n\nSeparating out the individual ensemble member output processing often results in more readable code.\n\nfunction process_member_data(simdir::SimDir)\n    isempty(simdir.vars) && return NaN\n    rsut =\n        get(simdir; short_name = \"rsut\", reduction = \"average\", period = \"30d\")\n    return slice(average_xy(rsut); time = 30days).data\nend\n\nNow, we can set up the remaining experiment details:\n\nensemble size, number of iterations\nthe prior distribution\nthe observational data\n\nensemble_size = 30\nn_iterations = 7\nnoise = 0.1 * I\nprior = constrained_gaussian(\"astronomical_unit\", 6e10, 1e11, 2e5, Inf)\n\nFor a perfect model, we generate observations from the forward model itself. This is most easily done by creating an empty parameter file and running the 0th ensemble member:\n\n@info \"Generating observations\"\nparameter_file = CAL.parameter_path(output_dir, 0, 0)\nmkpath(dirname(parameter_file))\ntouch(parameter_file)\nsimulation = CAL.forward_model(0, 0)\n\nLastly, we use the observation map itself to generate the observations.\n\nobservations = Vector{Float64}(undef, 1)\nobservations .= process_member_data(SimDir(simulation.output_dir))\n\nNow we are ready to run our calibration, putting it all together using the calibrate function. The WorkerBackend will automatically use all workers available to the main Julia process. Other backends are available for forward models that can't use workers or need to be parallelized internally. The simplest backend is the JuliaBackend, which runs all ensemble members sequentially and does not require Distributed.jl. For more information, see the Backends page.\n\neki = CAL.calibrate(\n    CAL.WorkerBackend(),\n    ensemble_size,\n    n_iterations,\n    observations,\n    noise,\n    prior,\n    output_dir,\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"quickstart/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"Every calibration requires\n\na forward model, which uses input parameters to return diagnostic output\nobservational data, which can be a Vector or an EnsembleKalmanProcess.Observation\na prior parameter distribution. The easiest way to construct a distribution is with the EnsembleKalmanProcess.constrained_gaussian function.","category":"section"},{"location":"quickstart/#Implementing-your-experiment","page":"Getting Started","title":"Implementing your experiment","text":"","category":"section"},{"location":"quickstart/#Forward-Model","page":"Getting Started","title":"Forward Model","text":"Your forward model must implement the forward_model(iteration, member)  function stub.\n\nSince this function only takes in the iteration and member numbers, there are some  hooks to obtain parameters and the output directory:\n\npath_to_ensemble_member returns the ensemble member's output directory, \n\nwhich can be used to set the forward model's output directory.\n\nparameter_path returns the ensemble member's parameter file, which can \n\nbe loaded in via TOML or passed to ClimaParams.","category":"section"},{"location":"quickstart/#Observational-data","page":"Getting Started","title":"Observational data","text":"Observational data generally consists of a vector of observations with length d  and the covariance matrix of the observational noise with size d  d.\n\nIf you need to stack or sample from observations, EnsembleKalmanProcesses.jl's  Observation  or ObservationSeries are fully-featured.\n\nAn observation map to process model output and return the full ensemble's observations is also required.\n\nThis is provided by implementing the function stub observation_map(iteration). This function needs to return an Array arr where arr[:, i] will return the i-th ensemble member's observational output.\n\nHere is a readable template for the observation_map\n\nfunction observation_map(iteration)\n    single_observation_dims = 1\n    G_ensemble = Array{Float64}(undef, single_observation_dims..., ensemble_size)\n    for member in 1:ensemble_size\n        G_ensemble[:, member] = process_member_data(iteration, member)\n    end\n    return G_ensemble\nend","category":"section"},{"location":"quickstart/#Optional-postprocessing","page":"Getting Started","title":"Optional postprocessing","text":"It may be the case that observation_map is insufficient as you need to more information, such as information from the ekp object to compute G_ensemble. Further postprocessing of the G_ensemble object can be done by implementing the postprocess_g_ensemble as shown below.\n\nfunction postprocess_g_ensemble(ekp, g_ensemble, prior, output_dir, iteration)\n    return g_ensemble\nend\n\nAfter each evaluation of the observation map and before updating the ensemble, it may be helpful to print the errors from the ekp object or plot G_ensemble. This can be done by implementing the analyze_iteration as shown below.\n\nfunction ClimaCalibrate.analyze_iteration(\n    ekp,\n    g_ensemble,\n    prior,\n    output_dir,\n    iteration,\n)\n    @info \"Analyzing iteration\"\n    @info \"Iteration $iteration\"\n    @info \"Current mean parameter: $(EnsembleKalmanProcesses.get__mean_final(prior, ekp))\"\n    @info \"g_ensemble: $g_ensemble\"\n    @info \"output_dir: $output_dir\"\n    return nothing\nend","category":"section"},{"location":"quickstart/#Parameters","page":"Getting Started","title":"Parameters","text":"Every parameter that is being calibrated requires a prior distribution to sample from.\n\nEnsembleKalmanProcesses.jl's constrained_gaussian  provides a user-friendly way to constructor Gaussian distributions.\n\nMultiple distributions can be combined using combine_distributions(vec_of_distributions).\n\nFor more information, see the EKP documentation for prior distributions.","category":"section"},{"location":"quickstart/#Experiment-Configuration","page":"Getting Started","title":"Experiment Configuration","text":"A calibration consisting of m ensemble members will run for n iterations.\n\nA good rule of thumb is an ensemble size 10 times the number of parameters.","category":"section"},{"location":"quickstart/#Calibrate","page":"Getting Started","title":"Calibrate","text":"Now all of the pieces should be in place:\n\nforward map\nobservation map\nobservations\ncovariance matrix of the observations (noise)\nprior distribution\nensemble size\nnumber of iterations\n\nAnd we can put it all together:\n\ncalibrate(ensemble_size, n_iterations, observations, noise, prior, output_dir)\n\nLastly, you need to set the output directory, ensemble size and the number of iterations to run for. A good rule of thumb for your ensemble size is 10x the number of free parameters.\n\nn_iterations = 7\nensemble_size = 10\noutput_dir = \"output/my_experiment\"\n\nOnce all of this has been set up, you can call put it all together using the calibrate function:\n\ncalibrate(ensemble_size, n_iterations, observations, noise, prior, output_dir)\n\nFor more information on parallelizing your calibration, see the Backends page.","category":"section"},{"location":"quickstart/#Checkpointing","page":"Getting Started","title":"Checkpointing","text":"ClimaCalibrate checkpoints each forward model and iteration so that an interrupted calibration can seamlessly pick up where it left off without wasting resources.\n\nIf a calibration (run via calibrate) exits after completing an iteration,  when it is restarted it will automatically run the next iteration.  This is done by checking if the ensemble forward map results file (G_ensemble.jld2)  and the EKI file (eki_file.jld2) have been saved.\n\nIf a calibration is interrupted during forward model execution,  causing a partial iteration, incomplete forward models will be rerun when the  calibration is restarted. Completed forward models will not be rerun. This is done by checking each model's checkpoint file and the flag it contains.","category":"section"},{"location":"quickstart/#Example-Calibrations","page":"Getting Started","title":"Example Calibrations","text":"The example tutorial provides a clear calibration example that can be run locally.\n\nAnother example experiment can be found in the package repo under experiments/surface_fluxes_perfect_model. This experiment uses the SurfaceFluxes.jl package  to generate a physical model that calculates the Monin Obukhov turbulent surface  fluxes based on idealized atmospheric and surface conditions. Since this is a \"perfect  model\" example, the same model is used to generate synthetic observations using its  default parameters and a small amount of noise. These synthetic observations are  considered to be the ground truth, which is used to assess the model ensembles'  performance when parameters are drawn from the prior parameter distributions. \n\nIt is a perfect-model calibration, using its own output as observational data.  By default, it runs 20 ensemble members for 6 iterations.  This example can be run on the most common backend, the WorkerBackend, with the following script:\n\nusing ClimaCalibrate\n\ninclude(joinpath(pkgdir(ClimaCalibrate), \"experiments\", \"surface_fluxes_perfect_model\", \"utils.jl\"))\n@show ensemble_size n_iterations observation variance prior\n\neki = calibrate(\n    ensemble_size,\n    n_iterations,\n    observation,\n    variance,\n    prior,\n    output_dir,\n)\n\ntheta_star_vec =\n    (; coefficient_a_m_businger = 4.7, coefficient_a_h_businger = 4.7)\n\nconvergence_plot(\n    eki,\n    prior,\n    theta_star_vec,\n    [\"coefficient_a_m_businger\", \"coefficient_a_h_businger\"],\n)\n\ng_vs_iter_plot(eki)","category":"section"},{"location":"observation_recipe/#ObservationRecipe","page":"Observation Recipes","title":"ObservationRecipe","text":"warning: Warning\nTo enable this module, use using ClimaAnalysis or import ClimaAnalysis.\n\nWhen handling weather and climate data, it can be tedious and error-prone when setting up the observation for calibration with EnsembleKalmanProcesses (or EKP for short). As such, ClimaCalibrate provides recipes for setting up observations consisting of samples, noise covariances, names, and metadata.","category":"section"},{"location":"observation_recipe/#How-do-I-use-this-to-set-up-observation-for-calibration-with-EKP?","page":"Observation Recipes","title":"How do I use this to set up observation for calibration with EKP?","text":"All functions assume that any data preprocessing is done with ClimaAnalysis.","category":"section"},{"location":"observation_recipe/#Covariance-Estimators","page":"Observation Recipes","title":"Covariance Estimators","text":"There are currently three covariance estimators, ScalarCovariance, SeasonalDiagonalCovariance, and SVDplusDCovariance, which are subtypes of AbstractCovarianceEstimator. ScalarCovariance approximates the observation noise covariance as a scalar diagonal matrix. SeasonalDiagonalCovariance approximates the observation noise covariance as a diagonal of variances across all the seasons for each observation, neglecting correlations between points. SVDplusDCovariance additionally approximates the correlations between points from, often limited, time series observations.","category":"section"},{"location":"observation_recipe/#Necessary-data-preprocessing","page":"Observation Recipes","title":"Necessary data preprocessing","text":"The OutputVars should represent time series data of summary statistics. For example, to compute seasonal averages of a OutputVar, one can use ClimaAnalysis.average_season_across_time, which will produce a OutputVar that can be used with either SeasonalDiagonalCovariance or SVDplusDCovariance.\n\nimport ClimaAnalysis\n\nobs_var = ClimaAnalysis.OutputVar(\n    \"precip.mon.mean.nc\",\n    \"precip\",\n    new_start_date = start_date,\n    shift_by = Dates.firstdayofmonth,\n)\n\n# -- preprocessing for units, times, grid, etc. --\n\nseasonal_averages = ClimaAnalysis.average_season_across_time(obs_var)","category":"section"},{"location":"observation_recipe/#Observation","page":"Observation Recipes","title":"Observation","text":"After preprocessing the OutputVars so that they represent time series data of summary statistics, one can use set up an EKP.observation as shown below.\n\nimport ClimaAnalysis\nimport EnsembleKalmanProcesses as EKP\nimport ClimaCalibrate\nimport ClimaCalibrate.ObservationRecipe\n\n# Vars are OutputVars preprocessed to ensure consistent units, times,\n# and grid as the diagonstics produced from the model.\n# In this example, we want to calibrate with seasonal averages, so we use\n# ClimaAnalysis.average_season_across_time\nvars = ClimaAnalysis.average_season_across_time.(vars)\n\n# We want the covariance matrix to be Float32, so we change it here.\nvars = ObservationRecipe.change_data_type.(vars, Float32)\n\n# We choose SVDplusDCovariance. We need to supply the start and end dates of\n# the samples with `sample_date_ranges`. To do this, we can use the function\n# below. In this example, the dates in `vars` are all the same. For debugging,\n# it is helpful to use `ClimaAnalysis.dates(var)`.\nsample_date_ranges =\n    ObservationRecipe.seasonally_aligned_yearly_sample_date_ranges(first(vars))\ncovar_estimator = SVDplusDCovariance(\n    sample_date_ranges,\n    model_error_scale = Float32(0.05),\n    regularization = Float32(1e-6),\n)\n\n# Finally, we form the observation\nstart_date = sample_date_ranges[1][1]\nend_date = sample_date_ranges[1][2]\nobs = ObservationRecipe.observation(\n    covar_estimator,\n    vars,\n    start_date,\n    end_date,\n)","category":"section"},{"location":"observation_recipe/#Metadata","page":"Observation Recipes","title":"Metadata","text":"note: Note\nMetadata in EKP.observation is only added with versions of EnsembleKalmanProcesses later than v2.4.2.\n\nWhen creating an observation with ObservationRecipe.observation, metadata is extracted from the OutputVars and attached to the observation. The metadata for each observation can be accessed with EKP.get_metadata(obs::EKP.Observation) and the metadata for each iteration can be accessed with ObservationRecipe.get_metadata_for_nth_iteration. The metadata can be used with ClimaAnalysis.unflatten to reconstruct the original OutputVar before flattening. See the ClimaAnalysis documentation about ClimaAnalysis.FlatVar for more information.","category":"section"},{"location":"observation_recipe/#Debugging-observational-and-simulation-data","page":"Observation Recipes","title":"Debugging observational and simulation data","text":"When setting up a calibration, it may be helpful to visualize the EKP.Observations or inspect the observational data and metadata together. To help with this, ObservationRecipe provides the functions ObservationRecipe.reconstruct_diag_cov and ObservationRecipe.reconstruct_vars. The function reconstruct_diag_cov reconstructs the diagonal of the covariance matrix as a vector of OutputVars, and the function reconstruct_vars reconstructs the samples of the EKP.Observation as a vector of OutputVars. To visualize the data in the OutputVars, see the visualize section in ClimaAnalysis.\n\nObservationRecipe.reconstruct_vars(obs)\n# Reconstructing the diagnonal of a covariance matrix as a OutputVar is only\n# supported for diagonal covariance matrices\nObservationRecipe.reconstruct_diag_cov(obs)\n\nFinally, ObservationRecipe provides a helper function for reconstructing the mean forward map evaluation with ObservationRecipe.reconstruct_g_mean_final. This can be helpful when debugging whether the G ensemble matrix is formed correctly or not.","category":"section"},{"location":"observation_recipe/#Frequently-asked-questions","page":"Observation Recipes","title":"Frequently asked questions","text":"Q: I need to compute g_ensemble and I do not know how the data of the OutputVars is flattened.\n\nA: When forming the sample, the data in a OutputVar is flattened using ClimaAnalysis.flatten. See ClimaAnalysis.flatten in the ClimaAnalysis documentation for more information. The order of the variables in the observation is the same as the order of the OutputVars when creating the EKP.Observation using ObservationRecipe.observation. If you are using ObservationRecipe, it is recommended that you also use GEnsembleBuilder which simplifies building the G ensemble matrix.\n\nQ: How do I handle NaNs in the OutputVars so that there are no NaNs in the sample and covariance matrix?\n\nA: NaNs should be handled when preprocessing the data. In some cases, there will be NaNs in the data (e.g. calibrating with data that is valid only over land). In these cases, the functions for making observations will automatically remove NaNs from the data. It is important to ensure that across the time slices, the NaNs appear in the same coordinates of the non-temporal dimensions. For example, if the quantity is defined over the dimensions longitude, latitude, and time, then any slice of the data at a particular longitude and latitude should either only contain NaNs or no NaNs at all.\n\nQ: How is the name of the observation determined?\n\nA: The name of the observation is determined by the short name in the attributes of the OutputVar. If there are multiple OutputVars, then the name is all the short names separated by semicolons. If no short name is found, then the name will be nothing.\n\nQ: What is regularization and model_error_scale when making a covariance matrix?\n\nA: The model error scale and regularization terms are used to inflate the diagonal of the observation covariance matrix to reflect estimates of measurement error. You can add a fixed percentage inflation of the noise due to the model error to the covariance matrix with the model_error_scale keyword argument. Additionally, to prevent very small variance along the diagonal of the covariance matrix, you can add a regularization with the regularization keyword argument.","category":"section"},{"location":"#ClimaCalibrate.jl","page":"Home","title":"ClimaCalibrate.jl","text":"ClimaCalibrate provides a scalable framework for calibrating forward models models using  the Ensemble Kalman Process (EKP). It integrates with EnsembleKalmanProcesses.jl  to enable distributed model calibration with minimal boilerplate code.\n\nKey Features\n\nDistributed computing support for multiple HPC environments\nIntegration with EnsembleKalmanProcesses.jl for parameter estimation\nFlexible model interface for different component models\nSupport for emulation and sampling workflows\n\nFor more information, see our Getting Started page.","category":"section"}]
}
