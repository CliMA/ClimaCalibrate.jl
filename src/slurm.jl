
export kwargs, slurm_model_run, wait_for_jobs

# Initial code is common to PBS and Slurm schedulers

"""
    kwargs(; kwargs...)

Create a dictionary from keyword arguments.
"""
kwargs(; kwargs...) = Dict{Symbol, Any}(kwargs...)

"""
    wait_for_jobs(jobids, output_dir, iter, experiment_dir, model_interface, module_load_str, model_run_func; verbose, hpc_kwargs, reruns=1)

Wait for a set of jobs to complete. If a job fails, it will be rerun up to `reruns` times.

In addition to scheduler status, a job is only considered successful when the model writes its
`completed` checkpoint file. This makes restarts robust when jobs exit early due to time limits
and are requeued by the scheduler.

Arguments:
- `jobids`: Vector of job IDs.
- `output_dir`: Directory for output.
- `iter`: Iteration number.
- `experiment_dir`: Directory for the experiment.
- `model_interface`: Interface to the model.
- `module_load_str`: Commands to load necessary modules.
- `model_run_func`: Function to run the model.
- `verbose`: Print detailed logs if true.
- `hpc_kwargs`: HPC job parameters.
- `reruns`: Number of times to rerun failed jobs.
"""
function wait_for_jobs(
    jobids::AbstractVector,
    output_dir,
    iter,
    experiment_dir,
    model_interface,
    module_load_str,
    model_run_func;
    verbose,
    hpc_kwargs,
    reruns = 0,
)
    rerun_job_count = zeros(length(jobids))
    completed_jobs = Set{Int}()

    try
        while length(completed_jobs) < length(jobids)
            for (m, jobid) in enumerate(jobids)
                m in completed_jobs && continue

                if job_failed(jobid)
                    log_member_error(output_dir, iter, m, verbose)
                    if rerun_job_count[m] < reruns

                        @info "Rerunning ensemble member $m"
                        jobids[m] = model_run_func(
                            iter,
                            m,
                            output_dir,
                            experiment_dir,
                            model_interface,
                            module_load_str;
                            hpc_kwargs,
                        )
                        rerun_job_count[m] += 1
                    else
                        push!(completed_jobs, m)
                    end
                elseif job_success(jobid)
                    # Only mark success if the model has written its completion checkpoint
                    if model_completed(output_dir, iter, m)
                        @info "Ensemble member $m complete"
                        push!(completed_jobs, m)
                    else
                        # The scheduler may report COMPLETED for a batch script that requeued itself.
                        # Wait until the completion file exists.
                        @debug "Job $jobid completed but checkpoint not found yet for member $m"
                    end
                end
            end
            sleep(5)
        end
    catch e
        kill_job.(jobids)
        if !(e isa InterruptException)
            @error "Pipeline crashed outside of a model run. Stacktrace:" exception =
                (e, catch_backtrace())
        end
    end

    report_iteration_status(jobids, output_dir, iter)
    return map(job_status, jobids)
end

"""
    log_member_error(output_dir, iteration, member, verbose=false)

Log a warning message when an error occurs. If verbose, includes the ensemble member's output.
"""
function log_member_error(output_dir, iteration, member, verbose = false)
    member_log = path_to_model_log(output_dir, iteration, member)
    warn_str = """Ensemble member $member raised an error. See model log at \
    $(abspath(member_log)) for stacktrace"""
    if verbose
        stacktrace = replace(readchomp(member_log), "\\n" => "\n")
        warn_str = warn_str * ": \n$stacktrace"
    end
    @warn warn_str
end

job_running(status::Symbol) = status == :RUNNING
job_success(status::Symbol) = status == :COMPLETED
job_failed(status::Symbol) = status == :FAILED
job_pending(status::Symbol) = status == :PENDING
job_completed(status::Symbol) = job_failed(status) || job_success(status)

job_pending(jobid) = job_pending(job_status(jobid))
job_running(jobid) = job_running(job_status(jobid))
job_success(jobid) = job_success(job_status(jobid))
job_failed(jobid) = job_failed(job_status(jobid))
job_completed(jobid) = job_completed(job_status(jobid))

"""
    report_iteration_status(jobids, output_dir, iter)

Report the status of an iteration. See also [`wait_for_jobs`](@ref).
"""
function report_iteration_status(jobids, output_dir, iter)
    if !all(job_completed.(jobids))
        error("Some jobs are not complete: $(filter(job_completed, jobids))")
    elseif all(job_failed, jobids)
        error(
            """Full ensemble for iteration $iter has failed. See model logs in
$(abspath(path_to_iteration(output_dir, iter)))""",
        )
    elseif any(job_failed, jobids)
        @warn "Failed ensemble members: $(findall(job_failed, jobids))"
    end
end

# Slurm-specific functions

"""
    submit_slurm_job(sbatch_filepath; env=deepcopy(ENV))

Submit a job to the Slurm scheduler using sbatch, removing unwanted environment variables.

Unset variables: "SLURM_MEM_PER_CPU", "SLURM_MEM_PER_GPU", "SLURM_MEM_PER_NODE"
"""
function submit_slurm_job(sbatch_filepath; env = ENV)
    clean_env = deepcopy(env)
    # List of SLURM environment variables to unset
    unset_env_vars = [
        "SLURM_MEM_PER_CPU",
        "SLURM_MEM_PER_GPU",
        "SLURM_MEM_PER_NODE",
        "SLURM_CPUS_PER_TASK",
        "SLURM_NTASKS",
        "SLURM_JOB_NAME",
        "SLURM_SUBMIT_DIR",
        "SLURM_JOB_ID",
    ]
    # Create a new environment without the SLURM variables
    for var in unset_env_vars
        delete!(clean_env, var)
    end

    try
        cmd = `sbatch --parsable $sbatch_filepath`
        output = readchomp(setenv(cmd, clean_env))
        # Parse job ID, handling potential format issues
        jobid = match(r"^\d+", output)
        if jobid === nothing
            error("Failed to parse job ID from output: $output")
        end

        return parse(Int, jobid.match)
    catch e
        error("Failed to submit SLURM job: $e")
    end
end

"""
    generate_sbatch_directives(hpc_kwargs)

Generate Slurm sbatch directives from HPC kwargs. 
"""
function generate_sbatch_directives(hpc_kwargs)
    @assert haskey(hpc_kwargs, :time) "Slurm kwargs must include key :time"

    hpc_kwargs[:time] = format_slurm_time(hpc_kwargs[:time])
    # Provide a default pre-timeout signal to the batch script unless user overrides
    if !haskey(hpc_kwargs, :signal)
        # Send SIGUSR1 to the batch script 5 minutes before time limit
        hpc_kwargs[:signal] = "B:USR1@300"
    end
    if !haskey(hpc_kwargs, :requeue)
        hpc_kwargs[:requeue] = true
    end

    slurm_directives = map(collect(hpc_kwargs)) do (k, v)
        key = replace(string(k), "_" => "-")
        if v === true
            return "#SBATCH --$key"
        elseif v === false
            return nothing
        else
            return "#SBATCH --$key=$(replace(string(v), "_" => "-"))"
        end
    end
    slurm_directives = filter(!isnothing, slurm_directives)
    return join(slurm_directives, "\n")
end
"""
    generate_sbatch_script(iter, member, output_dir, experiment_dir, model_interface; module_load_str, hpc_kwargs, exeflags="")
Generate a string containing an sbatch script to run the forward model.
`hpc_kwargs` is turned into a series of sbatch directives using [`generate_sbatch_directives`](@ref).
`module_load_str` is used to load the necessary modules and can be obtained via [`module_load_string`](@ref).
`exeflags` is a string of flags to pass to the Julia executable (defaults to empty string).
"""
function generate_sbatch_script(
    iter::Int,
    member::Int,
    output_dir,
    experiment_dir,
    model_interface,
    module_load_str,
    hpc_kwargs,
    exeflags = "",
)
    member_log = path_to_model_log(output_dir, iter, member)
    slurm_directives = generate_sbatch_directives(hpc_kwargs)
    ntasks = get(hpc_kwargs, :ntasks, 1)
    gpus_per_task = get(hpc_kwargs, :gpus_per_task, 0)
    climacomms_device = gpus_per_task > 0 ? "CUDA" : "CPU"
    # TODO: Remove this exception for GCP
    mpiexec_string =
        get_backend() == GCPBackend ?
        "/sw/openmpi-5.0.5/bin/mpiexec -n $ntasks" :
        "srun --output=$member_log --open-mode=append"
    sbatch_contents = """
    #!/bin/bash
    #SBATCH --job-name=run_$(iter)_$(member)
    #SBATCH --output=$member_log
    $slurm_directives

    # Self-requeue on pre-timeout or termination signals
    # `#SBATCH --signal=B:USR1@300` sends SIGUSR1 to the batch script 300 seconds before
    # the job time limit (B means send to the batch script). Sites may also deliver TERM.
    # We trap USR1/TERM and call `scontrol requeue $SLURM_JOB_ID` so the job returns to
    # the queue and can continue later with the same submission parameters.
    # Exiting with status 0 prevents a false failure due to the trap itself.
    trap 'echo "[ClimaCalibrate] Pre-timeout/TERM on job $SLURM_JOB_ID, requeuing"; scontrol requeue $SLURM_JOB_ID; exit 0' USR1 TERM

    $module_load_str
    export CLIMACOMMS_DEVICE="$climacomms_device"
    export CLIMACOMMS_CONTEXT="MPI"

    $mpiexec_string julia $exeflags --project=$experiment_dir -e '

        import ClimaCalibrate as CAL
        iteration = $iter; member = $member
        model_interface = "$model_interface"; include(model_interface)
        experiment_dir = "$experiment_dir"
        CAL.forward_model(iteration, member)
        CAL.write_model_completed("$output_dir", iteration, member)
    '
    exit 0
    """
    return sbatch_contents
end

"""
    slurm_model_run(iter, member, output_dir, experiment_dir, model_interface, module_load_str; hpc_kwargs)

Construct and execute a command to run a single forward model on Slurm.
Helper function for [`model_run`](@ref).
"""
function slurm_model_run(
    iter,
    member,
    output_dir,
    experiment_dir,
    model_interface,
    module_load_str;
    hpc_kwargs = Dict{Symbol, Any}(
        :time => 45,
        :ntasks => 1,
        :cpus_per_task => 1,
    ),
    exeflags = "",
)
    # Type and existence checks
    @assert isdir(output_dir) "Output directory does not exist: $output_dir"
    @assert isdir(experiment_dir) "Experiment directory does not exist: $experiment_dir"
    @assert isfile(model_interface) "Model interface file does not exist: $model_interface"

    # Range checks
    @assert iter >= 0 "Iteration number must be non-negative"
    @assert member > 0 "Member number must be positive"

    sbatch_contents = generate_sbatch_script(
        iter,
        member,
        output_dir,
        experiment_dir,
        model_interface,
        module_load_str,
        hpc_kwargs,
        exeflags,
    )

    jobid = mktemp(output_dir) do sbatch_filepath, io
        write(io, sbatch_contents)
        close(io)
        submit_slurm_job(sbatch_filepath)
    end
    return jobid
end

# Type alias for dispatching on PBSJobID (String) vs SlurmJobID (Int)
const SlurmJobID = Int

wait_for_jobs(
    jobids::AbstractVector{SlurmJobID},
    output_dir,
    iter,
    experiment_dir,
    model_interface,
    module_load_str;
    verbose,
    hpc_kwargs,
    reruns = 1,
) = wait_for_jobs(
    jobids,
    output_dir,
    iter,
    experiment_dir,
    model_interface,
    module_load_str,
    slurm_model_run;
    verbose = verbose,
    hpc_kwargs,
    reruns = reruns,
)

"""
    job_status(job_id)

Parse the slurm job_id's state and return one of three status symbols: :PENDING, :RUNNING, or :COMPLETED.
"""
function job_status(job_id::SlurmJobID)
    cmd = `squeue -j $job_id --format=%T --noheader`
    # Obtain stderr, difficult to do otherwise
    stdout = Pipe()
    stderr = Pipe()
    process = run(pipeline(ignorestatus(cmd), stdout = stdout, stderr = stderr))
    close(stdout.in)
    close(stderr.in)
    status = String(read(stdout))
    stderr = String(read(stderr))
    exit_code = process.exitcode

    # https://slurm.schedmd.com/job_state_codes.html
    pending_statuses = [
        "PENDING",
        "CONFIGURING",
        "REQUEUE_FED",
        "REQUEUE_HOLD",
        "REQUEUED",
        "RESIZING",
    ]
    running_statuses =
        ["RUNNING", "COMPLETING", "STAGED", "SUSPENDED", "STOPPED", "RESIZING"]
    failed_statuses = [
        "FAILED",
        "CANCELLED",
        "NODE_FAIL",
        "TIMEOUT",
        "OUT_OF_MEMORY",
        "PREEMPTED",
    ]
    invalid_job_err = "slurm_load_jobs error: Invalid job id specified"
    @debug job_id status exit_code stderr

    if status == ""
        # Not in squeue; fall back to sacct for a terminal state
        try
            acct = readchomp(`sacct -j $job_id --format=State%20 -n -X`)
            # sacct may return multiple lines; take the first non-empty token
            acct_state = strip(first(split(acct, '\n', keepempty = false), ""))
            if any(str -> occursin(str, acct_state), failed_statuses)
                return :FAILED
            elseif occursin("COMPLETED", acct_state)
                return :COMPLETED
            end
        catch
            # Ignore sacct errors and continue to other checks
        end
    end
    exit_code != 0 && contains(stderr, invalid_job_err) && return :COMPLETED

    any(str -> contains(status, str), pending_statuses) && return :PENDING
    any(str -> contains(status, str), running_statuses) && return :RUNNING
    any(str -> contains(status, str), failed_statuses) && return :FAILED

    @warn "Job ID $job_id has unknown status `$status`. Marking as completed"
    return :COMPLETED
end

"""
    kill_job(jobid::SlurmJobID)
    kill_job(jobid::PBSJobID)

End a running job, catching errors in case the job can not be ended.
"""
function kill_job(jobid::SlurmJobID)
    try
        run(`scancel $jobid`)
        println("Cancelling slurm job $jobid")
    catch e
        println("Failed to cancel slurm job $jobid: ", e)
    end
end

"Format `minutes` to a Slurm time string (D-HH:MM or HH:MM)"
function format_slurm_time(minutes::Int)
    days, remaining_minutes = divrem(minutes, (60 * 24))
    hours, remaining_minutes = divrem(remaining_minutes, 60)
    if days > 0
        return string(
            days,
            "-",
            lpad(hours, 2, '0'),
            ":",
            lpad(remaining_minutes, 2, '0'),
            ":00",
        )
    else
        return string(
            lpad(hours, 2, '0'),
            ":",
            lpad(remaining_minutes, 2, '0'),
            ":00",
        )
    end
end

format_slurm_time(str::AbstractString) = str
